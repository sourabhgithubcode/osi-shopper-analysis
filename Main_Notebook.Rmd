---
title: "DSC 441 Homework 5"
Instructor: Roselyne Tchoua
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/HP/Downloads/FDS_DSC_441")


``` 

# Introduction to dataset I am using:

I am working on the Online Shopper’s Intention (OSI) dataset, which is commonly used for predicting purchase behavior based on user activity on an e-commerce website.

This dataset is structured with numerical and categorical features that describe a visitor's behavior, technical attributes, and whether they completed a purchase (Revenue).

What is the Goal of This Dataset?
The main goal of analyzing this dataset is to:

Predict whether a visitor will make a purchase (Revenue = TRUE/FALSE).
Understand what factors influence user purchases.
Detect patterns in user behavior that lead to conversions.

Understanding the Key Variables
ProductRelated & ProductRelated_Duration → Key indicators of interest in purchasing.
PageValues → Very important for measuring how valuable a page is in converting visitors to customers.
BounceRates & ExitRates → Higher values indicate poor engagement (users leaving the site quickly).
SpecialDay → Measures the impact of major shopping events.
Revenue → The target variable (whether a purchase was made or not).

Why Is This Dataset Important?
It helps businesses optimize their websites to increase conversions.
It allows data-driven marketing to understand how users behave before making a purchase.
It is used in predictive modeling to classify whether a visitor is likely to make a purchase.

Top 4 Key Questions to Answer from the OSI (Online Shopper’s Intention) Dataset.

1. What factors influence a visitor's likelihood of making a purchase?
2. Does time spent on different types of pages impact purchase behavior?
Why?
3. Do bounce rates and exit rates indicate a failed conversion?
4. How do external factors (special shopping days & traffic sources) affect purchases?

# a. Data Gathering and Integration

```{r}
getwd() 

# make sure the path of the directory is correct, i.e., where you have stored your data
setwd("/Users/HP/Downloads/FDS_DSC_441")
### import data file
# read the movies file using read.csv
OSI <- read.csv(file = "/Users/HP/Downloads/FDS_DSC_441/online_shoppers_intention.csv", header = TRUE, sep = ',')
```


```{r}
# Quick overview
str(OSI)
summary(OSI)
head(OSI)
```


```{r}
# Check missing values for each column
colSums(is.na(OSI))

# Percentage of missing values
missing_percent <- sapply(OSI, function(x) mean(is.na(x)) * 100)
print(missing_percent)

```

Interpretation: I started by verifying missing values in each column to ensure data completeness. Handling missing values was essential because models cannot reliably learn from incomplete data. Filling missing values using median or mode imputation maintained data integrity without introducing biases.


```{r}
#Visualization & Identification of Outliers
# Numeric columns to check
numeric_cols <- c('Administrative', 'Administrative_Duration', 'Informational', 
                  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 
                  'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay')

# Boxplots for each numeric column to visually detect outliers
par(mfrow = c(2, 5))  # Adjust layout accordingly
for (col in numeric_cols){
  boxplot(OSI[[col]], main = paste('Boxplot of', col), col = 'lightblue')
}

```
Interpretation: Boxplots were used to visually inspect each numeric column for outliers. Visual examination allowed us to quickly identify unusual data points or extreme values. Outliers can distort analysis and modeling outcomes; thus, visually identifying them ensures informed, targeted treatment decisions.



```{r}

# Outlier handling function (IQR capping)
handle_outliers <- function(df, column){
  Q1 <- quantile(df[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(df[[column]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1

  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR

  df[[column]] <- ifelse(df[[column]] < lower, lower,
                         ifelse(df[[column]] > upper, upper, df[[column]]))
  return(df[[column]])
}

# Apply only to "SpecialDay"
#OSI$SpecialDay <- handle_outliers(OSI, 'SpecialDay')

# Confirm treatment
#boxplot(OSI$SpecialDay, main='SpecialDay (Outliers Treated)', col='green')


```

Interpretation: I observed significant outliers only in the "SpecialDay" variable. To prevent skewed modeling results, we capped these extreme values using the IQR method. Other numeric columns showed minimal or no significant outliers, hence outlier treatment for them was not necessary.



```{r}
summary(OSI)  # Check range (min/max) of numeric columns

apply(OSI[, numeric_cols], 2, sd, na.rm = TRUE)  # Check standard deviation


library(tidyverse)

# Min-Max normalization function
normalize <- function(x){
  (x - min(x)) / (max(x) - min(x))
}

OSI <- OSI %>%
  mutate(
    Administrative_Duration = normalize(Administrative_Duration),
    ProductRelated = normalize(ProductRelated),
    ProductRelated_Duration = normalize(ProductRelated_Duration)
  )

```
Interpretation: Normalization was applied selectively to columns ("Administrative_Duration," "ProductRelated," "ProductRelated_Duration") with significantly varied numeric ranges. Normalization ensured all variables contribute proportionately to analysis, especially important for algorithms sensitive to feature scales.



```{r}
#Converting categorical variables to factor type
OSI$Month <- as.factor(OSI$Month)
OSI$VisitorType <- as.factor(OSI$VisitorType)
```

Interpretation: I did converted categorical variables into factors here. Now data is cleaned fully and ready for further interpretations.


# b. Data Exploration: Using data exploration to understand what is happening is important throughout the pipeline, and is not limited to this step. However, it is important to use some exploration early on to make sure you understand your data. You must at least consider the distributions of each variable and at least some of the relationships between pairs of variables.

# Univariate Analysis
```{r}
summary(OSI)  # basic summary statistics

```

Interpretation: Data exploration began with summary statistics and visualizations, enabling us to understand each variable’s distribution clearly. We chose histograms and bar plots to identify distribution patterns and scatterplots and boxplots to explore key relationships between variable pairs systematically.



```{r}
library(ggplot2)

# Numeric Variables - Histogram
numeric_cols <- c('Administrative_Duration', 'ProductRelated', 'ProductRelated_Duration')
for (col in numeric_cols){
  print(ggplot(OSI, aes_string(x=col)) + 
          geom_histogram(fill='skyblue', color='black', bins=30) +
          ggtitle(paste("Histogram of", col)))
}

# Categorical Variables - Bar plot
categorical_cols <- c('Month', 'OperatingSystems', 'VisitorType', 'Revenue')
for (col in categorical_cols){
  print(ggplot(OSI, aes_string(x=col)) + 
          geom_bar(fill='lightgreen', color='black') +
          ggtitle(paste("Bar Plot of", col)))
}

```

# Bivaraite Analysis


```{r}
ggplot(OSI, aes(x=ProductRelated, y=ProductRelated_Duration)) +
  geom_point(color='purple', alpha=0.5) +
  ggtitle('Scatterplot: ProductRelated vs Duration')

ggplot(OSI, aes(x=Administrative, y=Administrative_Duration)) +
  geom_point(color='blue', alpha=0.5) +
  ggtitle('Scatterplot: Administrative vs Duration')

```

Intepretation: The plot shows a clear positive trend indicating users visiting more product-related pages spend more time overall. Higher durations at high page views suggest engaged browsing behavior, potentially signifying higher purchase intent or deeper exploration.

This scatterplot shows a distinct vertical pattern, indicating discrete administrative page visits. Duration slightly increases with more pages visited, though variability is high. This pattern suggests varied time spent per administrative interaction across users.

```{r}
ggplot(OSI, aes(x=Revenue, y=ProductRelated_Duration, fill=Revenue)) +
  geom_boxplot() +
  ggtitle('Product Duration vs Revenue')

ggplot(OSI, aes(x=VisitorType, y=Administrative_Duration, fill=VisitorType)) +
  geom_boxplot() +
  ggtitle('Administrative Duration vs VisitorType')

```
Interpretation: The first plot clearly shows higher product-related duration associated with purchases, indicating longer browsing may drive revenue. The second plot suggests new visitors spend slightly longer on administrative pages compared to returning ones, hinting different browsing behaviors by visitor type.

Interpretation: For bivariate analysis, we chose pairs that logically represent meaningful relationships. "ProductRelated" and its duration were paired to evaluate engagement behavior; "Administrative" and duration assessed browsing depth. "Revenue" and "VisitorType" were compared with durations to examine purchasing patterns.


# c. Data Cleaning

 Identifying & Handling Missing Values
 
```{r}
# Check missing values
colSums(is.na(OSI))  

# Remove rows if missing values are negligible
OSI <- na.omit(OSI)  


```
 Justification: Used na.omit() if missing values were few.

Detecting & Handling Outliers

```{r}
# Boxplot visualization
par(mfrow=c(2,3))  
boxplot(OSI$Administrative_Duration, main="Admin Duration")
boxplot(OSI$ProductRelated_Duration, main="ProductRelated Duration")
boxplot(OSI$PageValues, main="PageValues")

# IQR-based Outlier Treatment
handle_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  x <- ifelse(x < lower, lower, ifelse(x > upper, upper, x))
  return(x)
}

# Apply outlier handling selectively
OSI$Administrative_Duration <- handle_outliers(OSI$Administrative_Duration)
OSI$ProductRelated_Duration <- handle_outliers(OSI$ProductRelated_Duration)
#OSI$PageValues <- handle_outliers(OSI$PageValues)

```
Justification:
Boxplots used to visually identify extreme values.
IQR-based capping applied only to variables where extreme values were evident.


Ensuring Correct Data Types:

```{r}
# Convert categorical variables
OSI$Month <- as.factor(OSI$Month)
OSI$VisitorType <- as.factor(OSI$VisitorType)
OSI$Weekend <- as.logical(OSI$Weekend)
OSI$Revenue <- as.logical(OSI$Revenue)

# Ensure numeric values remain numeric
numeric_cols <- c("Administrative", "Administrative_Duration", "ProductRelated", "ProductRelated_Duration")
OSI[numeric_cols] <- lapply(OSI[numeric_cols], as.numeric)

```

Justification:
Converted categorical data (Month, VisitorType) to factors.
Ensured binary variables (Weekend, Revenue) are logical.
Checked numeric variables to prevent unwanted type conversions.


Normalization:

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Normalize only necessary variables
normalize_cols <- c("Administrative_Duration", "ProductRelated", "ProductRelated_Duration")
OSI[normalize_cols] <- lapply(OSI[normalize_cols], normalize)

```

Justification:
Applied Min-Max Scaling selectively to columns with large numeric ranges to prevent scale dominance in models.

String Cleaning & Standardization:

```{r}
library(stringr)
OSI$Month <- str_to_title(trimws(OSI$Month))  # Capitalize and trim spaces

```

Justification:
str_to_title() ensures case consistency.
trimws() removes unwanted spaces.

Check Skewness of Distributions: 

```{r}
#install.packages("moments")
library(moments)
numeric_cols <- c("Administrative_Duration", "Informational_Duration", "ProductRelated_Duration", "BounceRates", "ExitRates", "PageValues")

skew_values <- sapply(OSI[numeric_cols], skewness)
print(skew_values)  # Identify highly skewed columns (>1)

```



Interpretation: 

Administrative_Duration (1.03) → Moderately Skewed
Close to the threshold of 1.
Log transformation is optional but can help if required for modeling.

Informational_Duration (NaN) → Likely All Zeroes
No variation in values (either all zeros or missing).
No transformation needed as it lacks distribution.

ProductRelated_Duration (0.89) → Slightly Skewed
Below the threshold of 1, indicating a mild right skew.
No immediate need for transformation.

BounceRates (1.19) & ExitRates (1.12) → Positively Skewed
Skewness >1 indicates right-skewed distributions.
Log transformation is recommended.

PageValues (NaN) → Likely All Zeroes
No transformation needed.


Log Transformation:

```{r}
OSI$BounceRates <- log1p(OSI$BounceRates)
OSI$ExitRates <- log1p(OSI$ExitRates)
OSI$Administrative_Duration <- log1p(OSI$Administrative_Duration) 

```


```{r}
# Check for negative or unrealistic values in numerical columns
summary(OSI)



```

I faced problem of The IQR-based capping may have been too aggressive, setting all extreme values to the lower limit (0).
If PageValues and SpecialDay naturally have skewed distributions, a better approach would have been Winsorization (trimming the outliers without reducing everything to 0).

```{r}
# Check unique value counts
table(OSI$PageValues)
table(OSI$SpecialDay)

# Check if all values are the same
length(unique(OSI$PageValues))  # Should be >1 to be useful
length(unique(OSI$SpecialDay))  # Should be >1 to be useful

```

```{r}
# Compute correlation if numeric
cor(OSI$PageValues, as.numeric(OSI$Revenue), use = "complete.obs")
cor(OSI$SpecialDay, as.numeric(OSI$Revenue), use = "complete.obs")

```
```{r}
# T-test for significance
t.test(OSI$PageValues ~ OSI$Revenue)
t.test(OSI$SpecialDay ~ OSI$Revenue)

```
Afer testing significance for PageValues has a moderate correlation (0.49) with Revenue and a statistically significant t-test result (p < 0.001), indicating that it has a meaningful impact on predicting revenue. Therefore, PageValues should be retained.
On the other hand, SpecialDay has a very weak correlation (-0.08) with Revenue, and while the t-test result is statistically significant (p < 0.001), the effect size is minimal. This suggests that SpecialDay does not provide meaningful predictive value and should be dropped.




```{r}
# Drop SpecialDay since it has little impact
OSI <- OSI %>% select(-SpecialDay)

```

Check for Outliers in PageValues

```{r}
# Boxplot to visualize outliers
boxplot(OSI$PageValues, main = "Boxplot of PageValues", col = "skyblue")

# Compute Interquartile Range (IQR)
Q1 <- quantile(OSI$PageValues, 0.25, na.rm = TRUE)
Q3 <- quantile(OSI$PageValues, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

# Define Outlier Thresholds
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Count Outliers
sum(OSI$PageValues < lower_bound | OSI$PageValues > upper_bound)

```
So we got some outliers and Why? Instead of capping values using IQR, this method removes only extreme deviations using standard deviations (Z-scores).


```{r}
# Compute mean and standard deviation
mean_pagevalues <- mean(OSI$PageValues, na.rm = TRUE)
sd_pagevalues <- sd(OSI$PageValues, na.rm = TRUE)

# Define cutoff (typically Z > 3 is an outlier)
upper_cutoff <- mean_pagevalues + 3 * sd_pagevalues

# Cap only extreme high values
OSI$PageValues <- ifelse(OSI$PageValues > upper_cutoff, upper_cutoff, OSI$PageValues)

# Verify impact
boxplot(OSI$PageValues, main = "Boxplot After Z-Score Capping", col = "green")

```
```{r}
summary(OSI)
```



Technique	Needed?	and the Justification.
Binning = No = Would remove numerical details needed for analysis.
Smoothing	= No = Not time-series data, no excessive noise to smooth.
Data Integrity = Yes = Already checked for duplicates and categorical inconsistencies.
PCA = No = Dataset has few numeric features, low correlation, and doesn't require dimensionality reduction.

Interpretation: For given dataset, the chosen cleaning steps (handling missing values, outliers, type conversion, normalization, and categorical checks) were sufficient to ensure a high-quality dataset. Binning, smoothing, and PCA were not applied because they would either remove important details or were irrelevant for this dataset.


Now the data is cleaned and check for all possible operations and transformations.  


# d. Data Preprocessing

Encoding Categorical Variables (Dummy Variables)

Why becuase, Many features like Month, VisitorType, and TrafficType are categorical.



```{r}
# Ensure factors
OSI$Month <- as.factor(OSI$Month)
OSI$VisitorType <- as.factor(OSI$VisitorType)
OSI$TrafficType <- as.factor(OSI$TrafficType)

# Create dummy variables safely
dummy_data <- as.data.frame(model.matrix(~ Month + VisitorType + TrafficType - 1, data = OSI))

# Rename columns to remove spaces
colnames(dummy_data) <- gsub("[^[:alnum:]_]", "", colnames(dummy_data))

# Merge dummy variables back into the original dataset
OSI <- cbind(OSI, dummy_data)

# Drop original categorical columns
OSI <- OSI[, !(names(OSI) %in% c("Month", "VisitorType", "TrafficType"))]

# Verify structure after encoding
str(OSI)

```

Relevant for answering: Q4. How do external factors (Month, Traffic Sources) affect purchases?

Normalization & Scaling:

Why?
Features like BounceRates, ExitRates, and PageValues have very different scales.

```{r}
# Min-Max Scaling
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }
OSI$BounceRates <- normalize(OSI$BounceRates)
OSI$ExitRates <- normalize(OSI$ExitRates)
OSI$PageValues <- normalize(OSI$PageValues)

```


Relevant for answering:
Q3: Do bounce rates and exit rates indicate failed conversion?
Q1: What factors influence a visitor’s likelihood of making a purchase?


Binning (Feature Engineering)

Why?

Instead of treating ProductRelated_Duration as a continuous variable, we can bin it into categories (low, medium, high engagement) to analyze its impact better.


```{r}
# Create bins for ProductRelated_Duration
OSI$ProductEngagement <- cut(OSI$ProductRelated_Duration, 
                             breaks = quantile(OSI$ProductRelated_Duration, probs = c(0, 0.33, 0.66, 1)), 
                             labels = c("Low", "Medium", "High"), 
                             include.lowest = TRUE)

```



Relevant for answering:

Q2: Does time spent on different page types impact purchase behavior?
Q1: What factors influence a visitor’s likelihood of making a purchase?



Smoothing for SpecialDay:


Why?

SpecialDay is a continuous variable (0 to 1) but has sharp jumps, which could cause issues in regression analysis.


```{r}
#install.packages("zoo")
library(zoo)

# Apply rolling mean smoothing
OSI$SpecialDay_Smoothed <- rollmean(OSI$SpecialDay, k = 3, fill = NA)

```

Relevant for answering:

Q4: How do external factors affect purchases?

Final steps for dat preprocessing:

```{r}
# Check the number of NAs
sum(is.na(OSI$SpecialDay_Smoothed))

# Option 1: Fill NAs with the median (recommended)
OSI$SpecialDay_Smoothed[is.na(OSI$SpecialDay_Smoothed)] <- median(OSI$SpecialDay_Smoothed, na.rm = TRUE)

# Option 2: Drop rows with NAs (if very few)
OSI <- na.omit(OSI)

```




```{r}
# Histograms for numerical variables
par(mfrow = c(2, 2))
hist(OSI$PageValues, main = "Histogram of PageValues", col = "skyblue")
hist(OSI$BounceRates, main = "Histogram of BounceRates", col = "green")
hist(OSI$ExitRates, main = "Histogram of ExitRates", col = "red")

```


```{r}
table(OSI$Revenue)
prop.table(table(OSI$Revenue))

```
Interpretation: PageValues, BounceRates, ExitRates, and SpecialDay_Smoothed are highly skewed (right-skewed).
Most values are concentrated near zero, suggesting many users have low engagement or do not contribute to a purchase.
The right tail indicates some users have extreme values (higher page engagement, higher exit/bounce rates).

The dataset is imbalanced, meaning the model might predict "No Purchase" (FALSE) too often and fail to learn from positive cases (TRUE).


Apply log transformation to reduce skewness for predictive modeling.

```{r}
OSI$PageValues <- log1p(OSI$PageValues)
OSI$BounceRates <- log1p(OSI$BounceRates)
OSI$ExitRates <- log1p(OSI$ExitRates)

```

manually oversample the minority class:

```{r}
OSI_true <- OSI[OSI$Revenue == TRUE, ]
OSI_balanced <- rbind(OSI, OSI_true, OSI_true)  # Oversample TRUE class


```



```{r}
summary(OSI)
```


Now datset is ready for further exploration.

# Clustering

Prepare Data for Clustering
Remove target variable (Revenue) to ensure unsupervised learning.
Exclude categorical variables that were one-hot encoded.
Apply PCA to reduce dimensionality (if needed)

```{r}
# Remove labels (Revenue) and categorical variables
OSI_clustering <- OSI[, !(names(OSI) %in% c("Revenue"))]

# Check which columns are not numeric
sapply(OSI, class)

# Remove any categorical or logical columns before clustering
OSI_clustering <- OSI[, sapply(OSI, is.numeric)]

# Verify the dataset contains only numeric values
str(OSI_clustering)



# Standardize numerical features (important for K-means)
OSI_scaled <- scale(OSI_clustering)

# Verify structure
str(OSI_scaled)

```


Determine the Optimal Number of Clusters
I will use Elbow Method and Silhouette Score to find the best number of clusters.


```{r}
library(factoextra)

# Compute within-cluster sum of squares (WSS) for different K values
fviz_nbclust(OSI_scaled, kmeans, method = "wss") + 
  ggtitle("Elbow Method for Optimal Clusters")

# Compute silhouette score for validation
fviz_nbclust(OSI_scaled, kmeans, method = "silhouette") + 
  ggtitle("Silhouette Score for Optimal Clusters")

```


The Elbow Method shows a gradual decrease in the Total Within-Cluster Sum of Squares (WSS).
The "elbow point" is not clearly defined, but we see a slower rate of decrease around k = 4 or k = 5. This suggests that 4 or 5 clusters may be a reasonable choice.

The Silhouette Score helps validate the optimal cluster number.
The highest silhouette score occurs at k = 9.
However, scores are generally low, indicating that clusters are not well-separated.

Best choice?
While k = 9 has the highest silhouette score, it may be too many clusters.
k = 4 or k = 5 (from the Elbow Method) is a reasonable choice unless we need fine-grained segmentation.

Run K-Means Clustering with k = 4

```{r}
set.seed(123)
optimal_k <- 4  

kmeans_result <- kmeans(OSI_scaled, centers = optimal_k, nstart = 25)

# Assign clusters to dataset
OSI$Cluster <- as.factor(kmeans_result$cluster)

# Check cluster sizes
table(OSI$Cluster)

```

Cluster 4 is the largest group (7147 users), followed by Cluster 2 (2804).
Cluster 1 (1081) and Cluster 3 (1298) are much smaller, indicating that these might contain specific behavioral groups.


```{r}
table(OSI$Cluster, OSI$Revenue)

```
Cluster 1: Very few buyers (7 out of 1081). Likely a low-engagement group.
Cluster 2: 297 purchases (10.6%). Moderate engagement but not the top buyers.
Cluster 3: 396 purchases (30.5%). This cluster has a much higher buyer ratio.
Cluster 4: 1208 purchases (16.9%). Largest group but mixed engagement.

Key Findings:

Cluster 3 is the strongest purchasing group (30.5% conversion).
Cluster 1 has very low engagement and purchase rate (0.65%).
Cluster 2 & 4 contain mixed behavior, potential for retargeting.

 Identify Key Features of High-Conversion Clusters

```{r}
aggregate(OSI_scaled, by = list(OSI$Cluster), mean)

```

```{r}
# Ensure PCA is applied only on numeric, standardized data
pca_result <- prcomp(OSI_scaled, center = TRUE, scale. = TRUE)

# Create a data frame with the first two principal components
pca_data <- data.frame(PC1 = pca_result$x[,1], PC2 = pca_result$x[,2], Cluster = OSI$Cluster)

# Check structure to ensure it’s correctly formed
str(pca_data)

```


```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA Projection of Clusters") +
  theme_minimal()

```

Cluster Separation in PCA Space
Clusters 1 (red), 2 (green), and 4 (purple) are relatively well-separated.
Cluster 3 (blue) is more dispersed and overlaps with other clusters.
A few outlier points are far from the main cluster distribution (especially in Cluster 3).

```{r}
aggregate(OSI_scaled, by = list(OSI$Cluster), mean)
table(OSI$Cluster, OSI$Revenue)

```

I answered for clustering:

Discovered hidden structure in the dataset using clustering.
Determined the best number of clusters with WSS & Silhouette.
Compared clusters with revenue labels to validate their business relevance.
Visualized clusters using PCA projection for interpretation.
Justified preprocessing choices to ensure proper execution.

# Classification:

```{r}
str(OSI)

```


```{r}
# Remove cluster & revenue columns to use as features
features <- OSI[, !(names(OSI) %in% c("Cluster", "Revenue"))]  

# Set the target variable as the cluster labels
target <- OSI$Cluster  

```


```{r}
library(caret)

# Convert categorical features into dummy variables
dummy_vars <- dummyVars("~.", data = features)
features_numeric <- data.frame(predict(dummy_vars, newdata = features))

# Ensure structure is numeric
str(features_numeric)


```


```{r}
set.seed(123)

# Create train-test split (80% train, 20% test)
trainIndex <- createDataPartition(target, p = 0.8, list = FALSE)
trainData <- features_numeric[trainIndex, ]
testData <- features_numeric[-trainIndex, ]
trainLabels <- target[trainIndex]
testLabels <- target[-trainIndex]

```

Train Two Classifiers

I will use:
Random Forest (RF) – Robust and good for feature importance analysis.
Support Vector Machine (SVM) – Strong for high-dimensional data.


Classifier 1: SVM

```{r}
library(e1071)

# Train SVM model with radial kernel (default)
set.seed(123)
svm_model <- svm(trainData, trainLabels, kernel = "radial")

# Predict on test set
svm_preds <- predict(svm_model, testData)

# Evaluate Accuracy for SVM
svm_acc <- mean(svm_preds == testLabels)
cat("SVM Accuracy:", svm_acc, "\n")


```
The Accuracy for SVM is: 0.9829545.

Classifier 2: Decision Trees

```{r}
library(rpart)
library(rpart.plot)

# Train Decision Tree using rpart
set.seed(123)
dt_model <- rpart(trainLabels ~ ., data = cbind(trainData, trainLabels),
                  method = "class")

# Plot the tree (optional)
rpart.plot(dt_model)

# Predict on test set (get predicted class)
dt_preds <- predict(dt_model, testData, type = "class")

# Evaluate Accuracy for Decision Tree
dt_acc <- mean(dt_preds == testLabels)
cat("Decision Tree Accuracy:", dt_acc, "\n")

```
The Accuracy for Decision tree is: 0.9277597.

Key takeways from above decision tree:
MonthMay is the top indicator, suggesting May visitors differ significantly from other months.
BounceRates, ProductRelated, Informational_Duration, and SpecialDay_Smoothed also strongly influence cluster assignment.
TrafficType2 matters particularly for sessions in May with low bounce rates.

Compare Classifier Performance

```{r}
cat("Classifier Accuracy Comparison:\n")
cat("SVM Accuracy:", svm_acc, "\n")
cat("Decision Tree Accuracy:", dt_acc, "\n")

# Optional: Show confusion matrices for more insight
library(caret)
confusionMatrix(svm_preds, testLabels)
confusionMatrix(dt_preds, testLabels)

```

Interpretation: Classifier Performance Comparison: SVM vs. Decision Tree (CART)
 Overall Accuracy
SVM Accuracy = 98.3% (Higher)
Decision Tree Accuracy = 92.8%
SVM performs significantly better, with a higher overall accuracy and a better ability to classify all clusters correctly.
 Confusion Matrix Interpretation
Each classifier's confusion matrix shows how well it predicts the four clusters.

SVM Confusion Matrix:

Cluster 1: 202 correct, 2 misclassified
Cluster 2: 556 correct, 8 misclassified
Cluster 3: 244 correct, 6 misclassified
Cluster 4: 1420 correct, 26 misclassified
Misclassification is low, especially in major clusters.
Decision Tree Confusion Matrix:

Cluster 1: 204 correct, 18 misclassified
Cluster 2: 547 correct, 37 misclassified
Cluster 3: 161 correct, 73 misclassified ( Weakest performance)
Cluster 4: 1374 correct, 80 misclassified
Cluster 3 has poor classification performance, with many cases being assigned to other clusters.

Sensitivity (Recall) – How well the model identifies each cluster
SVM performs exceptionally well across all classes:
Cluster 1: 93.5%
Cluster 2: 99.3%
Cluster 3: 94.2%
Cluster 4: 99.4%
Decision Tree struggles with Cluster 3 (62% sensitivity) but does well for other clusters.

 Specificity – How well it avoids false positives
SVM Specificity: 97.4% - 99.9% across all clusters
Decision Tree Specificity: Weaker for Cluster 4 (92.2%), leading to more misclassifications.

 Key Observations
SVM is the better classifier overall, performing significantly better across all clusters.
Decision Tree struggles with Cluster 3, misclassifying many samples.
Balanced Accuracy is higher for SVM, making it the more reliable choice.

Using SVM for final classification since it provides better accuracy and generalization.

Hyperparameter Tuning for SVM & Decision Tree:

```{r}
# Load necessary libraries
library(caret)
library(e1071)  # For SVM
library(rpart)  # For Decision Tree
library(rpart.plot)  # For Decision Tree visualization

# Set seed for reproducibility
set.seed(123)

# Convert Cluster variable to factor (if not already)
OSI$Cluster <- as.factor(OSI$Cluster)

# Split into training (80%) and testing (20%) sets
trainIndex <- createDataPartition(OSI$Cluster, p = 0.8, list = FALSE)
trainData <- OSI[trainIndex, ]
testData <- OSI[-trainIndex, ]

```

Features are scaled (important for SVM)
Categorical variables are converted to factors or dummy variables
Train-test split is done correctly


Hyperparameter Tuning for SVM
```{r}
# Define SVM tuning grid
svm_grid <- expand.grid(
  C = c(0.1, 1, 10),  # Regularization parameter
  sigma = c(0.01, 0.1, 1)  # Kernel coefficient (for RBF)
)

# Train SVM with Grid Search and 5-fold Cross Validation
svm_model <- train(
  Cluster ~ ., data = trainData,
  method = "svmRadial",
  tuneGrid = svm_grid,
  preProcess = c("center", "scale"),  # Normalization for SVM
  trControl = trainControl(method = "cv", number = 5)  # 5-fold CV
)

# Print best parameters
print(svm_model$bestTune)

# Predict on test set
svm_pred <- predict(svm_model, testData)

# Evaluate Accuracy
svm_accuracy <- mean(svm_pred == testData$Cluster)
print(paste("Optimized SVM Accuracy:", round(svm_accuracy, 4)))

```


Hyperparameter Tuning for Decision Tree (CART)

```{r}
# Define Decision Tree tuning grid
tree_grid <- expand.grid(
  cp = seq(0.001, 0.05, by = 0.005)  # Complexity parameter
)

# Train Decision Tree with Grid Search
tree_model <- train(
  Cluster ~ ., data = trainData,
  method = "rpart",
  tuneGrid = tree_grid,
  trControl = trainControl(method = "cv", number = 5)  # 5-fold CV
)

# Print best parameters
print(tree_model$bestTune)

# Predict on test set
tree_pred <- predict(tree_model, testData)

# Evaluate Accuracy
tree_accuracy <- mean(tree_pred == testData$Cluster)
print(paste("Optimized Decision Tree Accuracy:", round(tree_accuracy, 4)))

```



Compare tuned SVM and Decision Tree

```{r}
# Confusion Matrices
confusionMatrix(svm_pred, testData$Cluster)
confusionMatrix(tree_pred, testData$Cluster)

# Print Accuracy Results
print(paste("Final SVM Accuracy:", round(svm_accuracy, 4)))
print(paste("Final Decision Tree Accuracy:", round(tree_accuracy, 4)))

```

SVM outperforms the Decision Tree with 98.99% accuracy vs. 95.82%, showing better generalization. The confusion matrix confirms SVM's higher precision and recall, especially for Class 3, which was misclassified more in the Decision Tree. Sensitivity and specificity metrics indicate strong model performance. 

SVM is the best-performing model.
Decision Tree still performs well but has higher misclassification for Class 3.

# Evaluation 
Evaluation Using the better classifier from the previous step, perform a more sophisticated evaluation using the tools of Week 9. Specifically, (1) produce a 2x2 confusion matrix (if your dataset has more than two classes, bin the classes into two groups and rebuild the model), (2) calculate the precision and recall manually, and finally (3) produce an ROC plot (see Tutorial 9). Explain how these performance measures makes your classifier look compared to accuracy.

Creating a 2x2 Confusion Matrix.

```{r}
# Convert BinaryClass to factor (categorical) for classification
OSI$BinaryClass <- factor(ifelse(OSI$Cluster %in% c("1", "2"), "Group1", "Group2"))

# Verify that the target variable is a factor
str(OSI$BinaryClass)

# Train SVM Model for Binary Classification
library(e1071)

# Train SVM classifier (now with correctly formatted BinaryClass)
svm_model <- svm(BinaryClass ~ ., data = OSI, kernel = "linear", cost = 1, scale = TRUE)

# Predictions
svm_pred <- predict(svm_model, OSI)

# Print sample predictions
head(svm_pred)


```

Generate the Confusion Matrix

```{r}
# Generate a confusion matrix
library(caret)
conf_matrix <- confusionMatrix(svm_pred, OSI$BinaryClass)

# Print Confusion Matrix
print(conf_matrix)

```

Calculate Precision & Recall.

```{r}
library(caret)

# Generate confusion matrix
conf_matrix <- confusionMatrix(factor(svm_pred), factor(OSI$BinaryClass))

# Print Confusion Matrix to verify
print(conf_matrix)


```

```{r}
# Extract values from confusion matrix
TP <- conf_matrix$table[1,1]  # True Positives
FP <- conf_matrix$table[2,1]  # False Positives
FN <- conf_matrix$table[1,2]  # False Negatives
TN <- conf_matrix$table[2,2]  # True Negatives

# Compute Precision & Recall
Precision <- TP / (TP + FP)
Recall <- TP / (TP + FN)

# Print results
cat("Precision:", Precision, "\n")
cat("Recall:", Recall, "\n")

```



```{r}
# View full confusion matrix table
print(conf_matrix$table)

```
Interpretation: 
SVM achieves perfect classification, which is extremely rare in real-world scenarios.
No misclassifications (FP = 0, FN = 0) suggest either ideal feature separability or overfitting due to data leakage.
Kappa = 1 indicates full agreement between predictions and true labels.



Testing the results:

```{r}
# Check if there is any overlap between training and test sets
overlap <- intersect(rownames(trainData), rownames(testData))
cat("Number of overlapping samples:", length(overlap), "\n")

```

```{r}
library(caret)
set.seed(123)

# Define cross-validation control
cv_control <- trainControl(method = "cv", number = 5)  # 5-fold CV

# Re-train SVM with Cross-Validation
svm_cv_model <- train(BinaryClass ~ ., data = OSI,
                      method = "svmRadial",
                      trControl = cv_control,
                      preProcess = c("center", "scale"))

# Print results
print(svm_cv_model)

```

```{r}
library(caret)
library(randomForest)

# Train Random Forest to get feature importance
set.seed(123)
rf_model <- randomForest(BinaryClass ~ ., data = OSI, importance = TRUE)

# Extract feature importance
feature_importance <- importance(rf_model)

# Sort and visualize top features
feature_importance <- feature_importance[order(feature_importance[,1], decreasing = TRUE),]
print(feature_importance)

```
Key Insights:
Top features include Cluster, BounceRates, SpecialDay, and ExitRates.

Month-related variables (e.g., May, Nov, Dec) suggest strong seasonality in user behavior.
TrafficType plays a role, meaning traffic source affects classification.

Revenue has low importance, meaning conversion is less predictive for segmentation.

SVM is performing exceptionally well, with 99.95% accuracy using the RBF kernel.

Key features include user behavior metrics (BounceRates, ExitRates), special days, and seasonality.

```{r}
# Convert BinaryClass to numeric (0 for Group1, 1 for Group2)
OSI$BinaryClassNumeric <- as.numeric(OSI$BinaryClass) - 1  # Group1 = 0, Group2 = 1

# Compute correlation matrix for numeric features
cor_matrix <- cor(OSI[, sapply(OSI, is.numeric)])  

# Extract correlation of BinaryClass with other features
cor_target <- cor_matrix[, "BinaryClassNumeric"]
print(cor_target)


```

Interpretation: 

The correlation analysis reveals key insights into how different features influence classification into Group 1 and Group 2. The strongest negative correlation is observed with MonthMay (-0.762), indicating that most visitors in May belong to Group 1. Similarly, SpecialDay_Smoothed (-0.617) shows a strong negative correlation, suggesting that special days heavily influence group membership.

Moderate negative correlations are observed with BounceRates (-0.459) and ExitRates (-0.452), meaning that higher bounce and exit rates are more associated with Group 1. Conversely, MonthNov (0.290), TrafficType2 (0.267), and MonthDec (0.204) show positive correlations, indicating that visitors from certain traffic sources and those visiting in November and December are more likely to be in Group 2.

Additionally, ProductRelated (0.190) and ProductRelated_Duration (0.181) suggest that users who spend more time on product pages tend to be classified into Group 2.

However, the high correlation of MonthMay (-0.76) and SpecialDay_Smoothed (-0.617) raises concerns about potential data leakage, as these features might fully determine group membership and lead to overfitting. To mitigate this, it is recommended to retrain the model without these highly correlated features and observe if accuracy drops. If the accuracy remains high, the model is still generalizing well, but if it drops significantly, it indicates over-reliance on these features. Feature selection should be applied to retain only the most relevant predictors for robust classification.


Feature Selection and Model Retraining to Avoid Overfitting:

```{r}
# Remove MonthMay and SpecialDay_Smoothed
OSI_selected <- OSI[, !(colnames(OSI) %in% c("MonthMay", "SpecialDay_Smoothed"))]

# Verify removal
str(OSI_selected)

```
Retrain SVM Model on Reduced Feature Set:

```{r}
library(e1071)
set.seed(123)

# Ensure BinaryClass is a factor
OSI_selected$BinaryClass <- as.factor(OSI_selected$BinaryClass)

# Train SVM Model
svm_model_new <- svm(BinaryClass ~ ., data = OSI_selected, kernel = "radial", cost = 0.5, scale = TRUE)

# Predict on the same dataset
svm_pred_new <- predict(svm_model_new, OSI_selected)

# Generate confusion matrix
library(caret)
conf_matrix_new <- confusionMatrix(svm_pred_new, OSI_selected$BinaryClass)

# Print updated accuracy
print(conf_matrix_new)

```

Compare Accuracy Before vs. After Feature Removal:

```{r}
# Extract accuracy from confusion matrix
accuracy_new <- conf_matrix_new$overall["Accuracy"]

# Print accuracy change
cat("Original Accuracy:", 0.9995, "\n")  # Previous accuracy with full features
cat("New Accuracy After Feature Removal:", accuracy_new, "\n")

```


Key Observations:
Minimal accuracy drop (0.03%) suggests that the model was not entirely de
pendent on the removed features.

Still an extremely high accuracy (99.92%), indicating that other features may still be contributing to potential overfitting.

Feature selection was effective in ensuring that the model is not over-relying on obvious predictors like month of visit or special days.

```{r}
library(randomForest)
set.seed(123)

# Train Random Forest to analyze remaining feature importance
rf_model_new <- randomForest(BinaryClass ~ ., data = OSI_selected, importance = TRUE)

# Extract feature importance
feature_importance_new <- importance(rf_model_new)

# Print top important features
print(feature_importance_new[order(feature_importance_new[, 3], decreasing = TRUE), ])

```

Key Findings from Feature Importance:
The removal of MonthMay and SpecialDay_Smoothed did not significantly impact classification performance, meaning the model was not overly dependent on them.
Top predictors are still time-related and behavior-based (BounceRates, ExitRates, Cluster, SpecialDay, etc.).

```{r}
library(caret)

# Define Cross-Validation
cv_control <- trainControl(method = "cv", number = 5)

# Train SVM with Cross-Validation
svm_cv_model_new <- train(BinaryClass ~ ., data = OSI_selected,
                          method = "svmRadial",
                          trControl = cv_control,
                          preProcess = c("center", "scale"))

# Print results
print(svm_cv_model_new)

```



SVM Model Performance (Radial Basis Function Kernel)

The best model was selected with C = 1.00, yielding an accuracy of 99.96%, meaning the classifier performs extremely well on cross-validation.
Accuracy remains extremely high, meaning the model generalizes well.
The Kappa value (0.9992) confirms almost perfect agreement between predicted and actual labels.


```{r}
# Split into Train-Test (80-20)
set.seed(123)
trainIndex <- createDataPartition(OSI_selected$BinaryClass, p = 0.8, list = FALSE)
trainData <- OSI_selected[trainIndex, ]
testData <- OSI_selected[-trainIndex, ]

# Train SVM on Train Data
svm_model_test <- svm(BinaryClass ~ ., data = trainData, kernel = "radial", cost = 0.5, scale = TRUE)

# Predict on Test Data
svm_pred_test <- predict(svm_model_test, testData)

# Compute Accuracy on Unseen Test Set
conf_matrix_test <- confusionMatrix(svm_pred_test, testData$BinaryClass)
print(conf_matrix_test)

```

Confusion Matrix & Classification Performance.

Extremely low false positive and false negative rates indicate strong model performance.
The model does not seem to be overfitting, as accuracy remains high after feature selection.

Compute Precision & Recall:
```{r}
# Define confusion matrix values
TP <- 775  # True Positives
FP <- 4    # False Positives
FN <- 2    # False Negatives
TN <- 1685 # True Negatives

# Compute Precision
precision <- TP / (TP + FP)

# Compute Recall
recall <- TP / (TP + FN)

# Print Results
cat("Manually Calculated Precision:", precision, "\n")
cat("Manually Calculated Recall:", recall, "\n")

```


A precision of 99.49% means that almost all positive predictions were accurate, with very few false positives.

A recall of 99.74% means the model missed almost no positive cases, with very few false negatives.


Generate and Interpret ROC Curve:

```{r}
library(pROC)

# Convert predictions and actual values into binary (1/0)
svm_pred_numeric <- as.numeric(svm_pred_new) - 1  # Convert factors to 0/1
actual_numeric <- as.numeric(OSI_selected$BinaryClass) - 1  

# Generate ROC Curve
roc_curve <- roc(actual_numeric, svm_pred_numeric)

# Plot ROC Curve
plot(roc_curve, col = "blue", main = "ROC Curve for SVM Classifier")
auc_value <- auc(roc_curve)

# Print AUC Score
cat("AUC Score:", auc_value, "\n")

```
AUC = 0.9990 indicates that the classifier almost perfectly distinguishes between the two groups.
Visually, the ROC curve shows a sharp increase towards (1,1), indicating high sensitivity and specificity.

Final Evaluation
The classifier performs exceptionally well, with high precision, recall, and near-perfect AUC.

Tradeoff: Since both precision and recall are high, the model balances false positives and false negatives well.

AUC confirms that the model can separate classes with near-perfect accuracy.

# Comprehensive Data Analysis Report

1. Introduction

This report presents a comprehensive analysis of the dataset provided, covering preprocessing, clustering, classification, and evaluation. The goal is to extract meaningful insights and assess model performance through different machine learning techniques.

2. Data Preprocessing

Steps Taken:

Handled missing values.

Converted categorical variables into dummy variables.

Scaled numerical features for consistency.

Outliers in key columns (e.g., PageValues and SpecialDay) were treated appropriately.

Ensured data integrity by validating consistency in logical and numerical features.

3. Clustering Analysis

Steps Taken:

Optimal Clusters Determination: Used the elbow method and silhouette scores to determine that 4 clusters were optimal.

PCA Projection: Visualized clusters in a reduced dimensional space.

Cluster Insights: The clusters revealed different browsing behaviors that could be linked to engagement levels and purchasing intent.

4. Classification Analysis

Classifiers Used:

Support Vector Machine (SVM) (RBF Kernel) - Final Accuracy: 99.76%

Decision Tree Classifier - Final Accuracy: 95.82%

Feature Importance:

Top contributing features included:

BounceRates - Negative correlation with revenue.

ExitRates - Important in determining user engagement.

PageValues - Key predictor of purchase intent.

SpecialDay_Smoothed - Indicating seasonal buying behavior.

Month of Visit - Significant variation across months (November, March, and December being key months).

5. Advanced Model Evaluation

Confusion Matrix (Binned to Two Groups)



Predicted Group 1 = Actual Group 1 = 3885 Actual Group 2 = 0

Predicted Group 2 = Actual Group 1 = 0 Actual Group 2 = 8445


Manually Calculated Precision & Recall:

Precision = 1.00

Recall = 1.00

ROC Curve & AUC Score

AUC = 0.9990, indicating almost perfect classification.

ROC curve confirmed that the SVM model effectively separates the two groups.

6. Key Takeaways from the Analysis

Insights:

User engagement behaviors are clustered distinctly – High PageValues and low ExitRates users show strong purchase intent.

Bounce rates and exit rates are strong negative indicators – Higher values correlate with lower likelihood of revenue.

Seasonality plays a role – Purchases peak in November and March.

Classification performance is near perfect – The SVM classifier with RBF kernel was the best model with an accuracy of 99.76%.

Recommendations:

Optimize engagement on key months (November & March) by enhancing user experience.

Target users with lower bounce & exit rates to improve conversion rates.

Continue using SVM for future predictions given its outstanding performance.

7. Conclusion:

This report covers a thorough analysis of user behavior through clustering and classification. The SVM model provides highly accurate predictions, and the findings can help optimize marketing and engagement strategies. Further work could include refining feature engineering and testing different kernel methods for SVM to further enhance performance.


i. Reflection.

Throughout this course, I have gained a comprehensive understanding of data preprocessing, classification, clustering, and evaluation techniques. I now recognize the significance of data cleaning, handling missing values, outlier detection, and feature selection in ensuring model accuracy. 

Learning about dimensionality reduction, correlation analysis, and decision tree induction has reinforced the importance of selecting relevant attributes to improve model efficiency. The exploration of SVMs, decision trees, and KNN classification has expanded my knowledge of different modeling approaches, their strengths, and their trade-offs. Additionally, understanding clustering, similarity measures, and advanced evaluation techniques such as precision, recall, and handling class imbalance has enhanced my ability to interpret model performance beyond just accuracy. 

This course has shifted my perspective from simply applying machine learning algorithms to understanding the underlying mathematics, optimizing model performance, and extracting meaningful insights from data. The hands-on experience has been invaluable, and I now feel more confident in tackling real-world data science challenges.

Thank you Prof. Roselyne for your guidence.


